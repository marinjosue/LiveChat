import pandas as pd
import numpy as np
import os
import pickle
import warnings
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_auc_score
)

warnings.filterwarnings('ignore')

print("\n" + "="*80)
print("VULNERABILITY DETECTOR - MODELO 1")
print("="*80)


# ================================================================================
# FASE 1: SAMPLE - Seleccionar y cargar datos
# ================================================================================

print("\n[FASE 1: SAMPLE]")
print("-" * 80)
print("Objetivo: Cargar dataset original y verificar estructura")

script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
dataset_path = os.path.join(project_root, 'data', 'processed', 'cybernative_detector_training.csv')

if not os.path.exists(dataset_path):
    print(f"ERROR: Dataset no encontrado en {dataset_path}")
    print("Asegurate de ejecutar prepare_dataset.py primero")
    exit(1)

print(f"Cargando dataset desde: {dataset_path}")
df = pd.read_csv(dataset_path)
print(f"Total registros: {len(df):,}")
print(f"Total columnas: {df.shape[1]}")

required_cols = ['codigo', 'lenguaje', 'vulnerable']
if not all(col in df.columns for col in required_cols):
    print(f"ERROR: Faltan columnas requeridas: {required_cols}")
    exit(1)

print(f"\nDistribucion de clases:")
class_counts = df['vulnerable'].value_counts()
for cls, count in class_counts.items():
    pct = (count / len(df)) * 100
    print(f"  - Vulnerable={cls}: {count:,} ({pct:.1f}%)")

print(f"\nLenguajes en dataset: {df['lenguaje'].nunique()}")
print(f"Valores nulos: {df.isnull().sum().sum()}")
print(f"Duplicados: {df.duplicated().sum()}")

print("\nFASE 1 completada")


# ================================================================================
# FASE 2: EXPLORE - Analisis exploratorio
# ================================================================================

print("\n[FASE 2: EXPLORE]")
print("-" * 80)
print("Objetivo: Entender caracteristicas del dataset")

print(f"\nEstadisticas de longitud de codigo:")
print(f"  - Promedio: {df['codigo'].str.len().mean():.0f} caracteres")
print(f"  - Minimo: {df['codigo'].str.len().min()} caracteres")
print(f"  - Maximo: {df['codigo'].str.len().max()} caracteres")

print(f"\nDistribucion de lenguajes:")
lang_dist = df['lenguaje'].value_counts()
for lang, count in lang_dist.items():
    pct = (count / len(df)) * 100
    print(f"  - {lang}: {count:,} ({pct:.1f}%)")

vulnerable_count = (df['vulnerable'] == 1).sum()
safe_count = (df['vulnerable'] == 0).sum()
diff = abs(vulnerable_count - safe_count)
print(f"\nBalanceo de clases:")
print(f"  - Diferencia: {diff} registros")
if diff < 100:
    print(f"  - Estado: BALANCEADO")
else:
    print(f"  - Estado: DESBALANCEADO")

print("\nFASE 2 completada")



# ================================================================================
# FASE 3: MODIFY - Transformar datos para modelado
# ================================================================================

print("\n[FASE 3: MODIFY]")
print("-" * 80)
print("Objetivo: Preparar features para el modelo")

X_code = df['codigo'].values
X_lang = df['lenguaje'].values
y = df['vulnerable'].values

print("\nFeature Engineering (TF-IDF):")
print("  - Max features: 1000")
print("  - N-grams: (1, 2)")
print("  - Vectorizando codigo...")

tfidf_vectorizer = TfidfVectorizer(
    max_features=1000,
    ngram_range=(1, 2),
    lowercase=True,
    stop_words='english',
    sublinear_tf=True,
    min_df=2,
    max_df=0.95
)

X_tfidf = tfidf_vectorizer.fit_transform(X_code)
print(f"  - Matriz TF-IDF shape: {X_tfidf.shape}")

print("\nFeature Engineering (Language Encoding):")
lang_encoder = LabelEncoder()
X_lang_encoded = lang_encoder.fit_transform(X_lang)
print(f"  - Lenguajes unicos: {len(lang_encoder.classes_)}")
print(f"  - Lenguajes: {', '.join(lang_encoder.classes_)}")

X_tfidf_dense = X_tfidf.toarray()
X_combined = np.column_stack([X_tfidf_dense, X_lang_encoded])
print(f"\nFeatures combinados shape: {X_combined.shape}")

print("\nDividiendo en train/test (80/20, stratified)...")
X_train, X_test, y_train, y_test = train_test_split(
    X_combined, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

print(f"  - Train: {len(X_train):,} muestras")
print(f"  - Test: {len(X_test):,} muestras")
print(f"  - Train class distribution: {np.bincount(y_train)}")
print(f"  - Test class distribution: {np.bincount(y_test)}")

print("\nFASE 3 completada")


# ================================================================================
# FASE 4: MODEL - Entrenar modelo
# ================================================================================

print("\n[FASE 4: MODEL]")
print("-" * 80)
print("Objetivo: Entrenar RandomForest para deteccion de vulnerabilidades")

print("\nConfiguracion del modelo:")
print("  - Algoritmo: RandomForestClassifier")
print("  - N estimators: 200")
print("  - Max depth: 25")
print("  - Min samples split: 5")
print("  - Random state: 42")

model = RandomForestClassifier(
    n_estimators=200,
    max_depth=25,
    min_samples_split=5,
    min_samples_leaf=1,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

print("\nEntrenando modelo...")
model.fit(X_train, y_train)
print("Entrenamiento completado")

print("\nFASE 4 completada")


# ================================================================================
# FASE 5: ASSESS - Evaluar modelo
# ================================================================================

print("\n[FASE 5: ASSESS]")
print("-" * 80)
print("Objetivo: Evaluar performance en datos de prueba")

print("\nGenerando predicciones...")
y_pred_train = model.predict(X_train)
y_pred_test = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

train_acc = accuracy_score(y_train, y_pred_train)
print(f"\nMETRICAS EN TRAIN SET:")
print(f"  - Accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)")

print(f"\nVALIDACION CRUZADA (5-Fold StratifiedKFold):")
kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
cv_scores = cross_val_score(model, X_combined, y, cv=kfold, scoring='accuracy', n_jobs=-1)
print(f"  - Fold 1: {cv_scores[0]:.4f}")
print(f"  - Fold 2: {cv_scores[1]:.4f}")
print(f"  - Fold 3: {cv_scores[2]:.4f}")
print(f"  - Fold 4: {cv_scores[3]:.4f}")
print(f"  - Fold 5: {cv_scores[4]:.4f}")
print(f"  - Promedio: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")

test_acc = accuracy_score(y_test, y_pred_test)
test_prec = precision_score(y_test, y_pred_test)
test_rec = recall_score(y_test, y_pred_test)
test_f1 = f1_score(y_test, y_pred_test)
test_roc = roc_auc_score(y_test, y_pred_proba)

print(f"\nMETRICAS EN TEST SET:")
print(f"  - Accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)")
print(f"  - Precision: {test_prec:.4f} ({test_prec*100:.2f}%)")
print(f"  - Recall:    {test_rec:.4f} ({test_rec*100:.2f}%)")
print(f"  - F1-Score:  {test_f1:.4f} ({test_f1*100:.2f}%)")
print(f"  - ROC-AUC:   {test_roc:.4f} ({test_roc*100:.2f}%)")

cm = confusion_matrix(y_test, y_pred_test)
tn, fp, fn, tp = cm.ravel()
print(f"\nMATRIZ DE CONFUSION (Test):")
print(f"  - True Negatives:  {tn}")
print(f"  - False Positives: {fp}")
print(f"  - False Negatives: {fn}")
print(f"  - True Positives:  {tp}")

overfit_diff = train_acc - test_acc
print(f"\nANALISIS DE OVERFITTING:")
print(f"  - Train Accuracy: {train_acc:.4f}")
print(f"  - Test Accuracy:  {test_acc:.4f}")
print(f"  - Diferencia:     {overfit_diff:.4f} ({overfit_diff*100:.2f}%)")
if overfit_diff < 0.10:
    print(f"  - Estado: ACEPTABLE (bajo overfitting)")
else:
    print(f"  - Estado: MODERADO (overfitting presente)")

print(f"\nREPORTE DE CLASIFICACION:")
print(classification_report(y_test, y_pred_test, 
                           target_names=['Seguro (0)', 'Vulnerable (1)']))

print("\nFASE 5 completada")


# ================================================================================
# GUARDAR ARTEFACTOS
# ================================================================================

print("\n[GUARDANDO ARTEFACTOS]")
print("-" * 80)

os.makedirs(os.path.join(project_root, 'models'), exist_ok=True)

model_path = os.path.join(project_root, 'models', 'vulnerability_detector.pkl')
with open(model_path, 'wb') as f:
    pickle.dump(model, f)
print(f"Modelo guardado: {model_path}")

vectorizer_path = os.path.join(project_root, 'models', 'vectorizer_detector.pkl')
with open(vectorizer_path, 'wb') as f:
    pickle.dump(tfidf_vectorizer, f)
print(f"Vectorizador guardado: {vectorizer_path}")

encoder_path = os.path.join(project_root, 'models', 'language_encoder.pkl')
with open(encoder_path, 'wb') as f:
    pickle.dump(lang_encoder, f)
print(f"Encoder guardado: {encoder_path}")

metrics_path = os.path.join(project_root, 'models', 'metrics_vulnerability_detector.txt')
with open(metrics_path, 'w') as f:
    f.write("VULNERABILITY DETECTOR - METRICAS FINALES\n")
    f.write("="*60 + "\n\n")
    f.write("DATASET:\n")
    f.write(f"  Total muestras: {len(df):,}\n")
    f.write(f"  Train: {len(X_train):,}\n")
    f.write(f"  Test: {len(X_test):,}\n\n")
    f.write("PERFORMANCE (TEST SET):\n")
    f.write(f"  Accuracy:  {test_acc:.4f}\n")
    f.write(f"  Precision: {test_prec:.4f}\n")
    f.write(f"  Recall:    {test_rec:.4f}\n")
    f.write(f"  F1-Score:  {test_f1:.4f}\n")
    f.write(f"  ROC-AUC:   {test_roc:.4f}\n\n")
    f.write("CONFUSION MATRIX:\n")
    f.write(f"  TN={tn}, FP={fp}, FN={fn}, TP={tp}\n\n")
    f.write("CONFIGURACION:\n")
    f.write(f"  Algoritmo: RandomForestClassifier\n")
    f.write(f"  N estimators: 200\n")
    f.write(f"  Max depth: 25\n")
    f.write(f"  Features: 1001 (1000 TF-IDF + 1 Language)\n")
print(f"Metricas guardadas: {metrics_path}")


# ================================================================================
# RESUMEN FINAL
# ================================================================================

print("\n" + "="*80)
print("RESUMEN DEL ENTRENAMIENTO")
print("="*80)

print(f"\nPERFORMANCE:")
print(f"  - Accuracy:  {test_acc*100:.2f}%")
print(f"  - ROC-AUC:   {test_roc*100:.2f}%")
print(f"  - Recall:    {test_rec*100:.2f}% (minimiza falsos negativos)")
